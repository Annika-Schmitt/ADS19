Applied Data Science
========================================================
author: Webscraping with R
date: 18.03.2019
autosize: false
width: 1920
height: 1080
font-family: 'Arial'
css: mySlideTemplate.css


<footer class = 'footnote'>
<div style="position: absolute; left: 0px; bottom: 50px; z-index:100; background-color:white">
Prof. Dr. Christoph Flath</div>
</footer>
<footer class = 'logo'>
<div style="position: absolute; left: 1100px; bottom: 25px; z-index:100; background-color:white">
<img src = "uni-wuerzburg-logo.svg" width="320">
</div>
</footer>


Sources
=======

https://blog.rsquaredacademy.com/web-scraping/

https://nceas.github.io/oss-lessons/data-liberation/intro-webscraping.html

https://stanford.edu/~vbauer/files/teaching/VAMScrapingSlides.html

<footer class = 'footnote'>
<div style="position: absolute; left: 0px; bottom: 0px; z-index:100; background-color:white">
Prof. Dr. Christoph Flath <br>ADS 2019</div>
</footer>
<footer class = 'logo'>
<div style="position: absolute; right: 0px; bottom: 0px; z-index:100; background-color:white">
<img src = "uni-wuerzburg-logo.svg" width="160">
</div>
</footer>


Web Scraping Overview
=======

* Web scraping is an umbrella term for automated extraction of information from websites

* In turn the entirety of WWW resources are turned into a source of potential data for many different projects in business and research


* We want to discuss how to get data from the World Wide Web using R

* After a few fundamental concepts we will spent most of the time on getting data from websites that do not offer any interface to automate information retrieval


***

*Key reasons for webscraping*

* Data Format: There is a wealth of data on websites but designed for human consumption. As such, we cannot use it for data analysis as it is not in a suitable format/shape/structure.
* No copy/paste: We cannot copy & paste the data into a local file. Even if we do it, it will not be in the required format for data analysis.
* No save/download: There are no options to save/download the required data from the websites. We cannot right click and save or click on a download button to extract the required data.
* Automation: With web scraping, we can automate the process of data extraction/harvesting.

<footer class = 'footnote'>
<div style="position: absolute; right: 250px; bottom: 0px; z-index:100; background-color:white">
Prof. Dr. Christoph Flath <br>ADS 2019</div>
</footer>
<footer class = 'logo'>
<div style="position: absolute; right: 0px; bottom: 0px; z-index:100; background-color:white">
<img src = "uni-wuerzburg-logo.svg" width="160">
</div>
</footer>


Available R Packages
=======

Relevant CRAN Task View: https://CRAN.R-project.org/view=WebTechnologies

*Major packages*

* `Rcurl` provides functions to fetch URIs, get & post forms
* `httr` interface for executing HTTP methods with support for modern web authentication protocols 
* `rvest` a higher level package which is simpler to use for basic tasks.
* `Rselenium` can be used to automate interactions and extract page content from dynamically generated webpages (i.e., those requiring user interaction to display results like clicking on button)

We will focus on `rvest`.

<footer class = 'footnote'>
<div style="position: absolute; right: 250px; bottom: 0px; z-index:100; background-color:white">
Prof. Dr. Christoph Flath <br>ADS 2019</div>
</footer>
<footer class = 'logo'>
<div style="position: absolute; right: 0px; bottom: 0px; z-index:100; background-color:white">
<img src = "uni-wuerzburg-logo.svg" width="160">
</div>
</footer>


Basics of Web Technology (1)
=========


* Uniform Resource Locators (URL) establish request messages which facilitate basic web communication. Build up from different components:
    * Protocol (`http` or `https`)
    * Domain (`www.xyz.dom`)
    * Port (`:1234`)
    * Path (`/here/be/data/`)
    * Request(`?value=xyz`)

* A domain controls automated extraction permissions by means of the file `robots.txt` in any path
    * We can check by means of the `robotstxt::paths_allowed()`
```{r}
library(robotstxt)
paths_allowed(
  paths = c("http://www.transfermarkt.de/","http://www.imdb.com/title/")
)
```
    

<footer class = 'footnote'>
<div style="position: absolute; right: 250px; bottom: 0px; z-index:100; background-color:white">
Prof. Dr. Christoph Flath <br>ADS 2019</div>
</footer>
<footer class = 'logo'>
<div style="position: absolute; right: 0px; bottom: 0px; z-index:100; background-color:white">
<img src = "uni-wuerzburg-logo.svg" width="160">
</div>
</footer>

Basics of Web Technology (2)
=========

To scrape website data some context of Web Technology and web site structures is helpful

A web page typically is made up of the following:

* HTML (Hyper Text Markup Language) takes care of the content. You need to have a basic knowledge of HTML tags as the content is typically located with these tags.
* CSS (Cascading Style Sheets) takes care of the appearance of the content. While you do not need to look into the CSS of a web page, you should be able to identify the id or class that manage the appearance of content.
* JS (Javascript) takes care of the behavior of the web page. Sites making heavy use of JS will require more careful scraping strategies.

<footer class = 'footnote'>
<div style="position: absolute; right: 250px; bottom: 0px; z-index:100; background-color:white">
Prof. Dr. Christoph Flath <br>ADS 2019</div>
</footer>
<footer class = 'logo'>
<div style="position: absolute; right: 0px; bottom: 0px; z-index:100; background-color:white">
<img src = "uni-wuerzburg-logo.svg" width="160">
</div>
</footer>

HTML and XML
=========

* HyperText Markup Language (HTML) describes and defines the content of a webpage

    * "Hyper Text" in HTML refers to links that connect webpages to one another, either within a single website or between websites

* HTML uses "markup" to annotate text, images, and other content for display in a Web browser. HTML markup includes special "elements"" such as `<head>`, `<title>`, `<body>`, `<header>`, `<footer>`, `<article>`, `<section>`,  `<p>`, `<div>`, `<span>`, `<img>`, and many others.

* Using you web browser, you can inspect the HTML content of any webpage of the World Wide Web.

***

* The eXtensible Markup Language (XML) provides a general approach for representing all types of information, such as data sets containing numerical and categorical variables
* XML provides the basic, common, and quite simple structure and syntax for all dialects or vocabularies
    * HTML, SVG and EML are specific vocabularies of XML.



<footer class = 'footnote'>
<div style="position: absolute; right: 250px; bottom: 0px; z-index:100; background-color:white">
Prof. Dr. Christoph Flath <br>ADS 2019</div>
</footer>
<footer class = 'logo'>
<div style="position: absolute; right: 0px; bottom: 0px; z-index:100; background-color:white">
<img src = "uni-wuerzburg-logo.svg" width="160">
</div>
</footer>

HTML Elements (1)
========

* HTML elements consist of a start tag and end tag with content inserted in between
* They can be nested and are case insensitive
* The tags can have attributes which usually come as name/value pairs
* When scraping web data we can use a combination of HTML tags and attributes to locate the content we want to extract



***

![Image](figures/htmltag.png)





<footer class = 'footnote'>
<div style="position: absolute; right: 250px; bottom: 0px; z-index:100; background-color:white">
Prof. Dr. Christoph Flath <br>ADS 2019</div>
</footer>
<footer class = 'logo'>
<div style="position: absolute; right: 0px; bottom: 0px; z-index:100; background-color:white">
<img src = "uni-wuerzburg-logo.svg" width="160">
</div>
</footer>

HTML Elements (2)
======
Below is a list of basic HTML which are helpful for web scraping

Tag | Description
--- | ---
`<html> </html>` | root node of html document
`<head> </head>` | head node of html document
`<title> </title>` | page title
`<body> </body>` | the body stores most the main page content
`<li> </li>` | list item
`<b> </b>` | bold font
`<p> </p>` | paragaph
`<img src="xxx"> </img>` | inserts image from source `xxx`
`<a href="xxx"> </a>` | links to destintation `xxx`

***

* The Document Object Model (DOM) defines the logical structure of a document and the way it is accessed and manipulated
* HTML is structured as a tree and you should be able to characterize the path to any node or tag



![Image](figures/dom_model.png)




<footer class = 'footnote'>
<div style="position: absolute; right: 250px; bottom: 0px; z-index:100; background-color:white">
Prof. Dr. Christoph Flath <br>ADS 2019</div>
</footer>
<footer class = 'logo'>
<div style="position: absolute; right: 0px; bottom: 0px; z-index:100; background-color:white">
<img src = "uni-wuerzburg-logo.svg" width="160">
</div>
</footer>


Scraping with `rvest`: Main functions and basic workflow
======
left: 35%

* Obtain an html document from a url, a file on disk or a string containing html with `read_html()`
* Select parts of a document using css selectors: `html_nodes(doc, "table td")` 
* Extract components with `html_tag()` (the name of the tag), `html_text()` (all text inside the tag), `html_attr()` (contents of a single attribute) and `html_attrs()` (all attributes) or parse tables into data frames with `html_table()`
* These functions can again be nicely stringed together using the pipe operator `%>%` - therefore we import both `rvest`and `tidyverse`

```{r}
library(tidyverse)
library(rvest)
```



***

```{r}
cities = "https://en.wikipedia.org/wiki/List_of_largest_cities"
paths_allowed(c(cities))
cities %>%
  read_html() %>%
  html_table(fill = TRUE, header = TRUE) %>%
  .[[2]] %>%
  .[, c(1,2,4)] %>%
  head(10)

```









<footer class = 'footnote'>
<div style="position: absolute; right: 250px; bottom: 0px; z-index:100; background-color:white">
Prof. Dr. Christoph Flath <br>ADS 2019</div>
</footer>
<footer class = 'logo'>
<div style="position: absolute; right: 0px; bottom: 0px; z-index:100; background-color:white">
<img src = "uni-wuerzburg-logo.svg" width="160">
</div>
</footer>



Selecting Nodes
=====
If the data is not explicitly organized as a table, we need to use formatting queues to isolate data points
Through html_node() we can specify css selectors or xpaths to select content on a website
Using http://selectorgadget.com/ this can be a very easy task – sometimes we need to try around

toniErdman = "http://www.imdb.com/title/tt4048272/?ref_=nv_sr_1"

toniErdman %>% 	read_html() %>%	html_nodes("#titleCast .itemprop span") %>%	html_text()


